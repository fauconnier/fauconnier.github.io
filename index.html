<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Jean-Philippe Fauconnier's Homepage">
        <meta name="keywords" content="Jean-Philippe Fauconnier, Computer Science,Natural Language Processing, Machine Learning, Artificial Intelligence">
        <meta name="author" content="Jean-Philippe Fauconnier">
        <meta http-equiv="X-UA-Compatible" content="chrome=1">
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

        <title>Jean-Philippe Fauconnier</title>
        <link rel="stylesheet" href="stylesheets/styles.css">
        <link rel="stylesheet" href="stylesheets/pygment_trac.css">

        <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->

      <!-- Clicky stats : same -->
      <script src="//static.getclicky.com/js" type="text/javascript"></script>
      <script type="text/javascript">try{ clicky.init(100964614); }catch(e){}</script>
      <noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/100964614ns.gif" /></p></noscript>

      <!-- include some js files -->
      <script src="javascripts/css_browser_selector.js" type="text/javascript"></script>
      <script src="javascripts/scale.fix.js"></script>

    </head>
    <body>
        <div class="wrapper">

            <!-- Header -->
            <header>
                <h1>Jean-Philippe Fauconnier</h1>
                <p>Currently, I am a Software Engineer at Apple </p> <p>Previously, I have obtained my PhD in Computer Science at the Université Paul Sabatier (Toulouse, France) and I have completed my Master Degree in Natural Language Processing at the Catholic University of Louvain (Belgium).</p>
                <ul>
                    <li><a href="#about">About</a></li>
                    <li><a href="#research">Research</a></li>
                    <li><a href="#works">Works</a></li>
                    <li><a href="#teaching">Teaching</a></li>
                    <li><a href="#software">Software</a></li>
                    <li><a href="#data">Data</a></li>
                    <li><a href="#experience">Experience</a></li>
                    <!--<li><a href="#links">Links</a></li>-->
                </ul>
            </header>

            <!-- Content -->
            <section>
                <!-- About -->
                <h3 id="firsth">About<span id="about" class="anchor"/></h3>
                <p>I have obtained my PhD in Computer Science at the <a href="http://www.irit.fr" target="_blank">Toulouse Institute of Computer Science Research</a>, Université Paul Sabatier (France). My work has taken place under the supervision of Drs <a href="http://www.irit.fr/~Mouna.Kamel/index_en.php" target="_blank">Mouna Kamel</a> and <a href="http://www.irit.fr/~Nathalie.Aussenac-Gilles/index_en.php" target="_blank">Nathalie Aussenac-Gilles</a>, and has focused on Document Analysis, Machine Learning and Natural Language Processing fields. More particularly, I was interested in the acquisition of lexical relations using the layout and the formatting of documents. Previously, I was graduated in Natural Language Processing at the <a href="http://www.uclouvain.be/en-index.html" target="_blank">Catholic University of Louvain</a> (Belgium) in 2012. This Master's degree programme was delivered by the <a href="http://www.uclouvain.be/en-cental.html" target="_blank">CENTAL</a> laboratory.</p>


                <!-- Research interests -->
                <h3>Research Interests<span id="research" class="anchor"/></h3>

                <h4>Natural Language Processing</h4>
                <ul>
                    <li><strong>Analysis</strong>: morpho-lexical analysis and syntactic parsing</li>
                    <li><strong>Corpora</strong>: development of annotation tools, annotation campaigns, and inter-rater agreement evaluation</li>
                    <li><strong>Meaning</strong>: ontology learning from text and Word embeddings</li>
                </ul>
                <h4>Document Analysis</h4>
                <ul>
                    <li><strong>Logical modeling</strong>: links between physical layout structures and logical ones</li>
                    <li><strong>LR Parsing</strong>: statistical and context-free grammar parsing for building logical tree</li>
                </ul>
                <h4>Statistics</h4>
                <ul>
                    <li><strong>Predictive modeling</strong>: supervised classification, structured prediction and unsupervised feature selection</li>
                    <li><strong>Descriptive analytics</strong>: statistical relationships and multivariate analysis </li>
                </ul>


                <!-- Works -->
                <h3>Works<span id="works" class="anchor"/></h3>

                <h4>National journal papers</h4>
                <ul>
                    <li>M. Kamel, B. Rothenburger, J.-P. Fauconnier. <i>Identification de relations sémantiques portées par les structures énumératives paradigmatiques : une approche symbolique et une approche par apprentissage supervisé</i>. Revue d'Intelligence Artificielle, Hermès Science, Numéro spécial Ingénierie des Connaissances. Nouvelles évolutions., Vol. 28, N. 2-3, p. 271-296, 2014. </li>
                </ul>
                <h4>International conference papers</h4>
                <ul>
                    <li>J.-P. Fauconnier, M. Kamel. <i>Discovering Hypernymy Relations using Text Layout</i> (regular paper). The Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), Denver, Colorado, 2015. <a href="http://embeddings.org/Fauconnier_Kamel_SEM2015.pdf">PDF</a> </li>
                    <li>J.-P. Fauconnier, M. Kamel, B. Rothenburger. <i>A Supervised Machine Learning Approach for Taxonomic Relation Recognition through Non-linear Enumerative Structures</i> (short paper). ACM Symposium on Applied Computing (SAC 2015), Salamanque, 2015. <a href="http://embeddings.org/Fauconnier_et_al_SWA2015.pdf">PDF</a> </li>
                    <li> J.-P. Fauconnier, M. Kamel, B. Rothenburger. <i>Une typologie multi-dimensionnelle des structures énumératives pour l'identification des relations termino-ontologiques</i> (regular paper). Conférence Internationale sur la Terminologie et l'Intelligence Artificielle (TIA 2013), Paris, Université Paris 13, p. 137-144, 2013. <a href="http://embeddings.org/Fauconnier_et_al_TIA2013.pdf">PDF</a></li>
                </ul>

                <h4>National conference papers</h4>
                <ul>
                    <li>J.-P. Fauconnier, L. Sorin, M. Kamel, Mustapha Mojahid, N. Aussenac-Gilles. <i>Détection automatique de la structure organisationnelle de documents à partir de marqueurs visuels et lexicaux</i> (regular paper). Traitement Automatique des Langues Naturelles (TALN 2014), Marseille, Association pour le Traitement Automatique des Langues (ATALA), p. 340-351, 2014. <a href="http://embeddings.org/Fauconnier_et_al_TALN2014.pdf">PDF</a> </li>
                    <li>J.-P. Fauconnier, M. Kamel, B. Rothenburger, N. Aussenac-Gilles. <i>Apprentissage supervisé pour l'identification de relations sémantiques au sein de structures énumératives parallèles</i> (regular paper). Traitement Automatique des Langues Naturelles (TALN 2013), Les Sables d'Olonne, Association pour le Traitement Automatique des Langues (ATALA), p. 132-145, 2013. <a href="http://embeddings.org/Fauconnier_et_al_TALN2013.pdf">PDF</a></li>
                </ul>

                <h4>Phd Thesis</h4>
                <ul>
                    <li>J.-P. Fauconnier, <i>Acquisition de liens sémantiques à partir d'éléments de mise en forme des textes : exploitation des structures énumératives</i> (PhD Thesis). Université de Toulouse, 2016. <a href="https://tel.archives-ouvertes.fr/tel-01324765/file/Fauconnier_2016.pdf">PDF</a> <a href="https://tel.archives-ouvertes.fr/tel-01324765">HAL</a> </li>
                </ul>

                <h4>Talks</h4>
                <ul>
                    <li>J.-P. Fauconnier. <i>La mise en forme des textes : un indice supplémentaire pour l'identification des relations hiérarchiques</i> (talk). Séminaire de l'équipe TALEP, Laboratoire d'Informatique Fondamentale de Marseille, 20-05-16.</li>
                    <li>J.-P. Fauconnier. <i>Mise en forme et indices linguistiques de surface pour l'extraction de connaissances</i> (talk). Journées d'étude internationales S'caladis, Université Toulouse Jean Jaurès, Toulouse, 19-11-15. <a href="http://w3.erss.univ-tlse2.fr/JE_Enumeration2015/Resumes/Fauconnier_JEenumeration2015.pdf">Abstract</a></li>
                    <li>J.-P. Fauconnier, M. Kamel, N. Aussenac-Gilles. <i>Acquisition de relations sémantiques à partir d’éléments de mise en forme des textes</i> (talk). Séminaires du CENTAL, Université Catholique de Louvain, Louvain-la-Neuve, 21-11-14. <a href="http://embeddings.org/Fauconnier_2014_CENTAL.pdf">PDF</a></li>
                    <li>J.-P. Fauconnier. <i>Métriques pour l’évaluation de l’annotation</i> (talk). Séminaires de l’équipe MELODI, Université Paul Sabatier, Toulouse, 25-11-13. <a href="http://www.slideshare.net/jfaucon/mtriques-pour-lvaluation-de-lannotation" target="_blank">Link</a></li>
                    <li>J.-P. Fauconnier, M. Kamel, N. Aussenac-Gilles. <i>A Supervised Learning for the Identification of Semantic Relations in Parallel Enumerative Structures</i> (poster). The 10th Summer School on Ontology Engineering and the Semantic Web (SSSW 2013), Cercédilla, 10-07-13. <a href="http://embeddings.org/Fauconnier_SSSW2013.pdf">PDF</a></li>
                    <li>J.-P. Fauconnier. <i>Classifieur d’Entropie Maximale (MaxEnt)</i> (talk). Séminaires de l’équipe MELODI, Université Paul Sabatier, Toulouse, 15-02-13. <a href="http://www.slideshare.net/jfaucon/max-ent" target="_blank">Link</a></li>
                    <li>A. Urieli, J.-P. Fauconnier. <i>PosTagger et Parseur Talismane</i> (talk). Séminaires de l’Axe TAL, CLLE-ERSS, Toulouse, 20-06-12. <a href="http://embeddings.org/Urieli_Fauconnier_2012.pdf">PDF</a></li>
                    <li>J.-P. Fauconnier, J. Roumier, F. Estiévenart. <i>Musonto - A Semantic Search Engine dedicated to Music and Musicians</i> (talk). Music Linked Data Workshop, JISC, Londres, 12-05-11. <a href="http://www.slideshare.net/MusicNet/jp-fauconnier-j-roumier-musonto-a-semantic-search-engine-dedicated-to-music-and-musicians" target="_blank">Link</a></li>
                </ul>


                <!-- Teaching -->
                <h3>Teaching<span id="teaching" class="anchor"/></h3>
                <h4>2015-2016</h4>
                <ul>
                    <li>Algorithms and C++ Programming - 54 hours - Université Jean Jaurès (Toulouse, France)</li>
                    <li>Relational Database - 44 hours - Université Jean Jaurès (Toulouse, France)</li>
                    <li>Web Integration - 40 hours - Université Jean Jaurès (Toulouse, France) </li>
                    <li>C2I Certification - 36 hours - Université Jean Jaurès (Toulouse, France) </li>
                    <li>XML technologies - 12 hours - Université Jean Jaurès (Toulouse, France)</li>
                    <li>Semantic Web - 4 hours - Université Jean Jaurès (Toulouse, France)</li>
                </ul>

                <h4>2014-2015</h4>
                <ul>
                    <li>Web Integration - 24 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                    <li>Relational Database - 18 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                    <li>Web Integration - 18 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                </ul>

                <h4>2013-2014</h4>
                <ul>
                    <li>Web Integration - 36 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                    <li>Relational Database - 12 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                    <li>Information System - 12 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                </ul>

                <h4>2012-2013</h4>
                <ul>
                    <li>Office Automation - 40 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                    <li>Office Automation - 24 hours - Institut Universitaire de Technologie (Tarbes, France)</li>
                </ul>

                <!-- Software -->
                <h3>Software<span id="software" class="anchor"/></h3>
                <p>Most of resources are located on my <a href="https://github.com/fauconnier?tab=repositories" class="user-mention" target="_blank">github repository</a>. The fast way to download a given <i>resource</i> is to use git:</p>
                <pre><code>mkdir resource
cd resource
git clone https://github.com/fauconnier/resource</code></pre>

                <h4>Personal softwares</h4>
                <a href="https://github.com/fauconnier/AMI" target="_blank"><img src="./pics/ami.png" alt="AMI" class="projectpic"/></a>
                <p><a href="https://github.com/fauconnier/AMI" target="_blank">AMI</a> (<i>Another Maxent Implementation</i>) is a R implementation of multinomial logistic regression, also known as Maximum Entropy classifier. This implementation deals with binary and real-valued features and uses standard R functions to optimize the objective. Then, it is possible to use several iterative methods: LM-BFGS, Conjugate Gradient, Gradient Descent and Generalized Iterative Scaling.</p>

                <a href="https://github.com/fauconnier/LARAt" target="_blank"><img src="./pics/larat.png" alt="LARAt" class="projectpic"/></a>
                <p><a href="https://github.com/fauconnier/LARAt" target="_blank">LARAt</a> (<i>Layout Annotation for Relation Acquisition tool</i>), pronounced /laʁa/, is an annotation tool which supports the layout and the formatting of HTML documents. LARAt was used during an annotation campaign in 2013 and, in his current state, is dedicated to the annotation of enumerative structures. The typology implemented is the one described in the TIA 2013 paper.</p>

                <a href="https://github.com/fauconnier/LaToe" target="_blank"><img src="./pics/latoe.png" alt="LaToe" class="projectpic"/></a>
                <p><a href="https://github.com/fauconnier/LaToe" target="_blank">LaToe</a> (<i>Layout Annotation for Textual Object Extraction</i>) is a tool which extracts the text layout from HTML, MediaWiki, or PDF documents for identifying specific textual objects (such as enumerative structures). Currently, the CRF model used for the PDF analyzer was trained on a small corpus (LING_GEOP). This implies that LaToe could be not efficient for unseen PDF documents with specific formatting. </p>


                <h4>Source code reviews</h4>
                <a href="https://github.com/fauconnier/code_review_tsuruoka" target="_blank"><img src="./pics/code_review_tsuruoka.png" alt="code_review_tsuruoka" class="projectpic"/></a>
                <p><a href="https://github.com/fauconnier/code_review_tsuruoka" target="_blank">Code review</a> of a <i>C++ library for maximum entropy classification</i>. On his website, <a href="http://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/" target="_blank">Tsuruoka</a> proposed a <a href="http://www.logos.ic.i.u-tokyo.ac.jp/~tsuruoka/maxent/" target="_blank">fast implementation</a> of a multinomial logistic regression. In order to get a better and deeper understanding of implementation details, I propose a simple code review. The code base is relatively small (around 2500 lines of code). Those notes are primary intended for my personal use and reflect my current understanding. I propose them here, in case it could help someone. Note that this document is currently a work in progress.</p>

                <h4>Open source contributions</h4>
                Some open source contributions:
                <ul>

                    <li><strong>2016</strong>
                    <ul>
                        <li><a href="https://github.com/fanff/wolfenscii" target="_blank">Wolfenscii</a> (<i>ASCII clone of Wolf 3D</i>) Added documentation. <a href="https://github.com/fanff/wolfenscii/pull/1" target="_blank">PR</a></li>

                        <li><a href="https://github.com/kassio/neoterm" target="_blank">neoterm</a> (<i>vim module</i>) Fixed tag into documentation. <a href="https://github.com/kassio/neoterm/pull/82" target="_blank">PR</a></li>

                        <li><a href="https://github.com/dkastner/sc" target="_blank">sc</a> (<i>spreadsheet calculator</i>) Updated sc 7.16. <a href="https://github.com/dkastner/sc/commit/d3ddd9687228f240fb3f66e0b7e4d4ccf5df012e" target="_blank">commit</a></li>

                        <li><a href="https://github.com/dkastner/sc" target="_blank">sc</a> (<i>spreadsheet calculator</i>) Fixed conflicting malloc. <a href="https://github.com/dkastner/sc/pull/1" target="_blank">PR</a></li>
                    </ul>
                    </li>

                    <li><strong>2015</strong>
                    <ul>
                        <li><a href="https://code.google.com/archive/p/lapdftext/" target="_blank">LAPDFText</a> (<i>Layout-Aware PDF Analyser</i>) Management of a student for improving bottom-up parsing and building a Swing GUI. <a href="https://github.com/fauconnier/lapdftext" target="_blank">fork</a></li>
                    </ul>
                    </li>



                    <li><strong>2014</strong>
                    <ul>
                        <li><a href="https://java.net/projects/frej" target="_blank">FREJ</a> (<i>Fuzzy Regular Expressions</i>) Client-server architecture for spreading the load across a cluster. <a href="https://github.com/fauconnier/fuzzymatcher-server" target="_blank">github</a></li>

                        <li><a href="http://search.cpan.org/%7Ethhamon/Lingua-YaTeA/" target="_blank">YaTeA</a> (<i>terminology extractor</i>) Adaptation to Talismane POS-tagset and Java client. <a href="https://github.com/fauconnier/yatea-client" target="_blank">github</a></li>

                        <li><a href="http://www.bdaille.com/index.php?option=com_content&task=blogcategory&id=5&Itemid=5&lang=en" target="_blank">ACABIT</a> (<i>terminology extractor</i>) Adaptation to Talismane POS-tagset and Java client. <a href="https://github.com/fauconnier/acabit-client" target="_blank">github</a> </li>

                        <li><a href="https://github.com/urieli/talismane" target="_blank">Talismane</a> (<i>statistical dependency parser for French</i>) Java client for treating corpora "on-the-fly".  <a href="https://github.com/fauconnier/talismane-client" target="_blank">github</a></li>
                    </ul>
                    </li>

                    <li><strong>2013-12</strong>
                    <ul>
                        <li><a href="https://github.com/urieli/talismane" target="_blank">Talismane</a> (<i>statistical dependency parser for French</i>) Minor fix.  <a href="https://github.com/urieli/talismane/commit/ec873be19b44367578320b5c5c3a03a207cc3eb4" target="_blank">commit</a></li>

                        <li><a href="https://github.com/urieli/talismane" target="_blank">Talismane</a> (<i>statistical dependency parser for French</i>) First version of user's manual. <a href="http://urieli.github.io/talismane/" target="_blank">last version</a></li>
                    </ul>
                    </li>

                </ul>

                <!-- Data -->
                <h3>Data<span id="data" class="anchor"/></h3>

                <h4>Word embeddings models<span id="wordembeddingmodels" class="anchor"/></h4>
                <p>I propose here some pre-trained word2vec models for French. Their format is the initial binary format proposed with  <a href="http://code.google.com/p/word2vec/" target="_blank">word2vec v0.1c</a>. Depending on your needs, you may want to convert those models. A simple way to convert them into text format can be: </p>
                <pre><code>git clone https://github.com/marekrei/convertvec
cd convertvec/
make
./convertvec bin2txt frWiki_no_phrase_no_postag_700_cbow_cut100.bin output.txt
</code></pre>
                <p>Below I give a minimal usage example in Python:</p>

                <pre><code>pip install word2vec
python
>>> import word2vec
>>> model = word2vec.load('frWac_postag_no_phrase_700_skip_cut50.bin')
>>> indexes, scores = model.cosine(<span style="color:red">'intéressant_a'</span>)
>>> model.generate_response(indexes,scores).tolist()
[(<span style="color:blue">'très_adv'</span>        , 0.5967900206395151),
(<span style="color:green">'intéresser_v'</span>     , 0.5439725695003301),
(<span style="color:blue">'peu_adv'</span>          , 0.542676993533696),
(<span style="color:blue">'assez_adv'</span>        , 0.5398579170306232),
(<span style="color:blue">'certainement_adv'</span> , 0.5246291122355085),
(<span style="color:blue">'plutôt_adv'</span>       , 0.5234975073833474),
(<span style="color:red">'instructif_a'</span>     , 0.5230028009476526),
(<span style="color:green">'trouver_v'</span>        , 0.5131327677418707),
(<span style="color:blue">'aussi_adv'</span>        , 0.5056422730726639),
(<span style="color:blue">'beaucoup_adv'</span>     , 0.5034801589883425)]
</code></pre>

<p>For this model, we can see that the adjective 'intéressant' has a lot of shared contexts with adverbs.  Note that the color code and the layout are mine.
Please check <a href="https://arxiv.org/abs/1310.4546" target="_blank">(Mikolov <i>et al., 2013)</i></a> to gain insight into the model hyper-parameters.</p>

                <h5>frWac2Vec</h5>
                <p><a href="http://wacky.sslmit.unibo.it/doku.php?id=corpora" target="_blank">FrWac corpus</a>, 1.6 billion of words. </p>
                <table>
                    <tr>
                        <td></td>
                        <td>Lem</td>
                        <td>Pos</td>
                        <td>Phrase</td>
                        <td>Train</td>
                        <td>Dim</td>
                        <td>Cutoff</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_non_lem_no_postag_no_phrase_200_cbow_cut0.bin">bin (2.7Gb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>200</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin">bin (120Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>200</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin">bin (120Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>200</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_non_lem_no_postag_no_phrase_500_skip_cut100.bin">bin (298Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>500</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_non_lem_no_postag_no_phrase_500_skip_cut200.bin">bin (202Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>500</td>
                        <td>200</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_no_postag_no_phrase_500_cbow_cut100.bin">bin (229Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>500</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_no_postag_no_phrase_500_skip_cut100.bin ">bin (229Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>500</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_no_postag_no_phrase_700_skip_cut50.bin">bin (494Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>700</td>
                        <td>50</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_postag_no_phrase_700_skip_cut50.bin">bin (577Mb)</a></td>
                        <td>&or;</td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>700</td>
                        <td>50</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_postag_no_phrase_1000_skip_cut100.bin">bin (520Mb)</a></td>
                        <td>&or;</td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>1000</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_no_postag_phrase_500_cbow_cut10.bin">bin (2Gb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>&or;</td>
                        <td>cbow</td>
                        <td>500</td>
                        <td>10</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWac_no_postag_phrase_500_cbow_cut100.bin">bin (289Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>&or;</td>
                        <td>cbow</td>
                        <td>500</td>
                        <td>100</td>
                    </tr>
                </table>

                <h5>frWiki2Vec</h5>
                <p><a href="https://dumps.wikimedia.org/frwiki/" target="_blank">FrWiki dump</a>  (<a href="http://embeddings.org/frWiki_non_lem.txt.gz">raw file</a>), 600 millions of words.</p>
                <table>
                    <tr>
                        <td></td>
                        <td>Lem</td>
                        <td>Pos</td>
                        <td>Phrase</td>
                        <td>Train</td>
                        <td>Dim</td>
                        <td>Cutoff</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_lem_no_postag_no_phrase_1000_cbow_cut100.bin">bin (253Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>1000</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_lem_no_postag_no_phrase_1000_cbow_cut200.bin">bin (195Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>1000</td>
                        <td>200</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_lem_no_postag_no_phrase_1000_skip_cut100.bin">bin (253Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>1000</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_lem_no_postag_no_phrase_1000_skip_cut200.bin">bin (195Mb)</a></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>1000</td>
                        <td>200</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_phrase_no_postag_500_cbow_cut10.bin">bin (128Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>500</td>
                        <td>10</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_phrase_no_postag_700_cbow_cut100.bin">bin (106Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>cbow</td>
                        <td>700</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_phrase_no_postag_1000_skip_cut100.bin">bin (151Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>1000</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td><a href="http://embeddings.org/frWiki_no_phrase_no_postag_1000_skip_cut200.bin">bin (121Mb)</a></td>
                        <td>&or;</td>
                        <td>-</td>
                        <td>-</td>
                        <td>skip</td>
                        <td>1000</td>
                        <td>200</td>
                    </tr>
                </table>

                </br>
                <h5>How to cite those models?</h5>
                <p>According to the licence  <a href="https://creativecommons.org/licenses/by/3.0/legalcode" target="_blank">CC-BY 3.0</a>, please, feel free to copy, distribute, remix and tweak those models for any purpose. The attribution must be made by a link to <a href="http://fauconnier.github.io">this page</a>. 
Those models were trained during my PhD Thesis, and are in no way linked to my current or future activities. Note also that those models are shared without any guarantees or support.</p>

                <p>Below, projects and papers using those models:</p>
                <ul>
					<li><i>Practical Deep Learning For Coders</i> (2017) &mdash; Lesson <a href="http://forums.fast.ai/t/lesson-13-wiki/2332" target="_blank">13</a> &mdash; <a href="http://www.fast.ai" target="_blank">Fast.ai</a> project</li>
                    <li>Rothe, S., Ebert, S. and Schütze, H. (2016). <i>Ultradense Word Embeddings by Orthogonal Transformation</i>. NAACL 2016. San Diego <a href="https://arxiv.org/abs/1602.07572" target="_blank">arXiv</a></li>
                    <li>Fauconnier, J.-P., Kamel, M. (2015). <i>Discovering Hypernymy Relations using Text Layout</i>. *SEM 2015. Denver. <a href="http://embeddings.org/Fauconnier_Kamel_SEM2015.pdf">PDF</a></li>
                </ul>
                <p>To see your work listed, contact <a href="mailto:jean.philippe.fauconnier@gm32il.com?subject=s/gm32il/gmail/">me</a>.</p>

                <br>
                <h4>Annotated copora</h4>
                <p>Annotated corpora build during my PhD thesis:</p>
                <ul>
                    <li><a href="https://github.com/fauconnier/corpus-LARA" target="_blank">corpus-LARA</a>: The corpus LARA is a French corpus of Wikipedia pages annotated with enumerative structures. </li>
                    <li><a href="https://github.com/fauconnier/corpus-LING_GEOP" target="_blank">corpus-LING_GEOP</a>: LING_GEOP is a corpus with visual and logical clues. The PDF documents come from <a href="http://redac.univ-tlse2.fr/corpus/annodis/me_download/index_en.html" target="_blank">ANNODIS-ME</a>.</li>
                </ul>


                <!-- Experience -->
                <h3>Experience<span id="experience" class="anchor"/></h3>
                <h4>Laboratory life</h4>
                <ul>
                    <li>Member of the <a href="http://www.irit.fr" target="_blank">IRIT</a> doctoral committee</li>
                    <li>Member of the <a href="http://jetou2015.free.fr/index-en.htm" target="_blank">JéTou 2015</a> organization committee </li>
                </ul>
                <h4>Review</h4>
                <ul>
                    <li>Subreviewer for <a href="http://iswc2016.semanticweb.org/" target="_blank">ISWC 2016</a> (<a href="http://rd.springer.com/book/10.1007/978-3-319-46523-4" target="_blank">Springer</a>)</li>

                    <li>Subreviewer for <a href="http://iswc2014.semanticweb.org/" target="_blank">ISWC 2014</a> (<a href="http://link.springer.com/book/10.1007%2F978-3-319-11964-9" target="_blank">Springer</a>)</li>
                    <li>Subreviewer for <a href="http://www.k-cap.org/kcap13/events.kmi.open.ac.uk/kcap2013/" target="_blank">K-CAP 2013</a> (<a href="http://dl.acm.org/citation.cfm?id=2479832" target="_blank">ACM</a>) </li>
                </ul>
                <h4>Jobs &amp; internships</h4>
                <ul>
                    <li>Axe TAL, <a href="http://w3.erss.univ-tlse2.fr/textes/operations/operation4-EN.html" target="_blank">CLEE-ERSS</a>, Université Toulouse II, 2012. Evaluation of the French  <a href="http://github.com/urieli/Talismane" target="_blank">Talismane</a> parser.</li>
                    <li>Division ICT, <a href="http://www.fagg-afmps.be/en/" target="_blank">Federal Agency for Medicines and Health Products</a>, Belgium, 2011. Treatment of <a href="https://eudract.ema.europa.eu/" target="_blank">EudraCT</a> data.</li>
                    <li>Software &amp; Technologies, <a href="https://www.cetic.be/spip.php?lang=en" target="_blank">CETIC</a>, Belgium, 2010. Development of the semantic search engine <a href="http://www.slideshare.net/MusicNet/jp-fauconnier-j-roumier-musonto-a-semantic-search-engine-dedicated-to-music-and-musicians#stats-panel" target="_blank">MusOnto</a>.</li>
                    <li>Division PRE, <a href="http://www.fagg-afmps.be/en/" target="_blank">Federal Agency for Medicines and Health Products</a>, Belgium, 2009. Database integrity verification.</li>
                </ul>

                <!-- Links -->
                <h3>Links<span id="links" class="anchor"/></h3>
                <ul>
                    <li><a href="https://github.com/fauconnier/" class="user-mention" target="_blank">Github profile</a></li>
                    <li><a href="http://www.quora.com/Jean-Philippe-Fauconnier/answers" target="_blank">Quora profile</a></li>
                </ul>

            </section>

            <!-- Footer -->
            <footer>
                <p><small>Hosted on <a href="http://github.com/fauconnier" target="_blank">GitHub</a> Pages &mdash; Original layout by <a href="https://github.com/orderedlist/minimal" target="_blank">orderedlist</a> &mdash; Slightly modified with <a href="http://www.vim.org/about.php" target="_blank">Vim</a></small></p>
                <!-- Vim, What Else? -->
            </footer>

        </div>
    </body>
</html>
